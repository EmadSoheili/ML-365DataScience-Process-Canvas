{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a112d48e-7564-448e-9a6f-0eb129e5ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12aad291-be08-4f12-a658-54f89b01b6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Vehicle_Age</th>\n",
       "      <th>Vehicle_Damage</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Policy_Sales_Channel</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; 2 Years</td>\n",
       "      <td>Yes</td>\n",
       "      <td>40454.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>33536.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; 2 Years</td>\n",
       "      <td>Yes</td>\n",
       "      <td>38294.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id Gender  Age  Driving_License  Region_Code  Previously_Insured  \\\n",
       "0   1   Male   44                1         28.0                   0   \n",
       "1   2   Male   76                1          3.0                   0   \n",
       "2   3   Male   47                1         28.0                   0   \n",
       "\n",
       "  Vehicle_Age Vehicle_Damage  Annual_Premium  Policy_Sales_Channel  Vintage  \\\n",
       "0   > 2 Years            Yes         40454.0                  26.0      217   \n",
       "1    1-2 Year             No         33536.0                  26.0      183   \n",
       "2   > 2 Years            Yes         38294.0                  26.0       27   \n",
       "\n",
       "   Response  \n",
       "0         1  \n",
       "1         0  \n",
       "2         1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "df = pd.read_csv('Cross_sell_prediction.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0f9da3-db79-4c0c-accf-0aef8d92339a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Policy_Sales_Channel</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "      <td>381109.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>190555.000000</td>\n",
       "      <td>38.822584</td>\n",
       "      <td>0.997869</td>\n",
       "      <td>26.388807</td>\n",
       "      <td>0.458210</td>\n",
       "      <td>30564.389581</td>\n",
       "      <td>112.034295</td>\n",
       "      <td>154.347397</td>\n",
       "      <td>0.122563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>110016.836208</td>\n",
       "      <td>15.511611</td>\n",
       "      <td>0.046110</td>\n",
       "      <td>13.229888</td>\n",
       "      <td>0.498251</td>\n",
       "      <td>17213.155057</td>\n",
       "      <td>54.203995</td>\n",
       "      <td>83.671304</td>\n",
       "      <td>0.327936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2630.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>95278.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24405.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>190555.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31669.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>285832.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39400.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>381109.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>540165.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id            Age  Driving_License    Region_Code  \\\n",
       "count  381109.000000  381109.000000    381109.000000  381109.000000   \n",
       "mean   190555.000000      38.822584         0.997869      26.388807   \n",
       "std    110016.836208      15.511611         0.046110      13.229888   \n",
       "min         1.000000      20.000000         0.000000       0.000000   \n",
       "25%     95278.000000      25.000000         1.000000      15.000000   \n",
       "50%    190555.000000      36.000000         1.000000      28.000000   \n",
       "75%    285832.000000      49.000000         1.000000      35.000000   \n",
       "max    381109.000000      85.000000         1.000000      52.000000   \n",
       "\n",
       "       Previously_Insured  Annual_Premium  Policy_Sales_Channel  \\\n",
       "count       381109.000000   381109.000000         381109.000000   \n",
       "mean             0.458210    30564.389581            112.034295   \n",
       "std              0.498251    17213.155057             54.203995   \n",
       "min              0.000000     2630.000000              1.000000   \n",
       "25%              0.000000    24405.000000             29.000000   \n",
       "50%              0.000000    31669.000000            133.000000   \n",
       "75%              1.000000    39400.000000            152.000000   \n",
       "max              1.000000   540165.000000            163.000000   \n",
       "\n",
       "             Vintage       Response  \n",
       "count  381109.000000  381109.000000  \n",
       "mean      154.347397       0.122563  \n",
       "std        83.671304       0.327936  \n",
       "min        10.000000       0.000000  \n",
       "25%        82.000000       0.000000  \n",
       "50%       154.000000       0.000000  \n",
       "75%       227.000000       0.000000  \n",
       "max       299.000000       1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numeric fields\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b9c9a2-60db-4dd3-bb5f-8e5bcb3b0000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9r/h_zpgkhs4q9296qjtg37jqlw0000gn/T/ipykernel_1569/128259272.py:3: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  df.describe(include=np.object)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Vehicle_Age</th>\n",
       "      <th>Vehicle_Damage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>381109</td>\n",
       "      <td>381109</td>\n",
       "      <td>381109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Male</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>206089</td>\n",
       "      <td>200316</td>\n",
       "      <td>192413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Gender Vehicle_Age Vehicle_Damage\n",
       "count   381109      381109         381109\n",
       "unique       2           3              2\n",
       "top       Male    1-2 Year            Yes\n",
       "freq    206089      200316         192413"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Non-numberic fields\n",
    "\n",
    "df.describe(include=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d22c6935-aa49-4aa8-919c-a242f7022f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                      0\n",
       "Gender                  0\n",
       "Age                     0\n",
       "Driving_License         0\n",
       "Region_Code             0\n",
       "Previously_Insured      0\n",
       "Vehicle_Age             0\n",
       "Vehicle_Damage          0\n",
       "Annual_Premium          0\n",
       "Policy_Sales_Channel    0\n",
       "Vintage                 0\n",
       "Response                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Null values\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "# No Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbcfb293-74e5-4ef6-98c8-046ca03846ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male      206089\n",
      "Female    175020\n",
      "Name: Gender, dtype: int64\n",
      "1-2 Year     200316\n",
      "< 1 Year     164786\n",
      "> 2 Years     16007\n",
      "Name: Vehicle_Age, dtype: int64\n",
      "Yes    192413\n",
      "No     188696\n",
      "Name: Vehicle_Damage, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for possible nulls in categoricals / non answers\n",
    "\n",
    "for i in df.select_dtypes(include=['object']).columns:\n",
    "    print(df[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b01c8e2-3087-4f9b-bd82-48310ee33c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152.0    134784\n",
       "26.0      79700\n",
       "124.0     73995\n",
       "160.0     21779\n",
       "156.0     10661\n",
       "          ...  \n",
       "149.0         1\n",
       "43.0          1\n",
       "144.0         1\n",
       "143.0         1\n",
       "41.0          1\n",
       "Name: Policy_Sales_Channel, Length: 155, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of Sales channels\n",
    "df.Policy_Sales_Channel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "129ba650-6b19-4a0a-ba1d-c69bd2dd841d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Vehicle_Age</th>\n",
       "      <th>Vehicle_Damage</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; 2 Years</td>\n",
       "      <td>Yes</td>\n",
       "      <td>40454.0</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>33536.0</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; 2 Years</td>\n",
       "      <td>Yes</td>\n",
       "      <td>38294.0</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender  Age  Driving_License  Previously_Insured Vehicle_Age Vehicle_Damage  \\\n",
       "0   Male   44                1                   0   > 2 Years            Yes   \n",
       "1   Male   76                1                   0    1-2 Year             No   \n",
       "2   Male   47                1                   0   > 2 Years            Yes   \n",
       "\n",
       "   Annual_Premium  Vintage  Response  \n",
       "0         40454.0      217         1  \n",
       "1         33536.0      183         0  \n",
       "2         38294.0       27         1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select some fields\n",
    "\n",
    "df_trimmed = df.loc[:,['Gender','Age','Driving_License','Previously_Insured','Vehicle_Age',\n",
    "                       'Vehicle_Damage','Annual_Premium','Vintage','Response']]\n",
    "df_trimmed.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13fe1c40-447d-4752-a3f9-2e4e645194e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Response</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Vehicle_Age_1-2 Year</th>\n",
       "      <th>Vehicle_Age_&lt; 1 Year</th>\n",
       "      <th>Vehicle_Age_&gt; 2 Years</th>\n",
       "      <th>Vehicle_Damage_No</th>\n",
       "      <th>Vehicle_Damage_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40454.0</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33536.0</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38294.0</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Driving_License  Previously_Insured  Annual_Premium  Vintage  \\\n",
       "0   44                1                   0         40454.0      217   \n",
       "1   76                1                   0         33536.0      183   \n",
       "2   47                1                   0         38294.0       27   \n",
       "\n",
       "   Response  Gender_Female  Gender_Male  Vehicle_Age_1-2 Year  \\\n",
       "0         1              0            1                     0   \n",
       "1         0              0            1                     1   \n",
       "2         1              0            1                     0   \n",
       "\n",
       "   Vehicle_Age_< 1 Year  Vehicle_Age_> 2 Years  Vehicle_Damage_No  \\\n",
       "0                     0                      1                  0   \n",
       "1                     0                      0                  1   \n",
       "2                     0                      1                  0   \n",
       "\n",
       "   Vehicle_Damage_Yes  \n",
       "0                   1  \n",
       "1                   0  \n",
       "2                   1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop null values and create dummy variables\n",
    "# No null values\n",
    "df_final = pd.get_dummies(df_trimmed).dropna()\n",
    "df_final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe73d740-5ba5-4b68-b27b-7edee34d64c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Driving_License', 'Previously_Insured', 'Annual_Premium',\n",
       "       'Vintage', 'Response', 'Gender_Female', 'Gender_Male',\n",
       "       'Vehicle_Age_1-2 Year', 'Vehicle_Age_< 1 Year', 'Vehicle_Age_> 2 Years',\n",
       "       'Vehicle_Damage_No', 'Vehicle_Damage_Yes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final columns\n",
    "\n",
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91334f13-e77e-45d3-bcb5-859f2b7ae7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    334399\n",
       "1     46710\n",
       "Name: Response, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the data is imbalance\n",
    "\n",
    "df_final.Response.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ad0451e-1160-4d80-95ec-500649237ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train test split \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_final.drop('Response', axis =1)\n",
    "y = df_final.loc[:,['Response']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b689a4e6-8a8b-463b-8e98-e0c517d73a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the data (SMOTE) Try this if interested: https://www.kaggle.com/code/kenjee/dealing-with-imbalanced-data-section-10\n",
    "# Commented out. It is hre just in case. But for now, we won't use it.\n",
    "\n",
    "\n",
    "#from imblearn.over_sampling import SMOTE \n",
    "#smote = SMOTE(sampling_strategy =1)\n",
    "\n",
    "#X_train, y_train = smote.fit_resample(X_train,y_train)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c0df75-6b5d-4344-acb8-910ec55b06b4",
   "metadata": {},
   "source": [
    "# Creating a Basline Model\n",
    "How can we tell if our machine learning models are any good? To evaluate performance, we need to benchmark against something. In this case, we will create two baslines for our model. First, we can simply look at the average of our data for a numeric value. If we were going to predict the age, we could simply guess the average age for every candidate. \n",
    "\n",
    "On the other hand, for a categorical variable, we could simply guess 50/50 or the ratio of the categories in the data. In this case, the conversion data is imbalanced with 46710/ 334399 samples being of the non-stroke cateogry. That means that if we guessed that everyone in the sample didn't have a stroke, we would have a 86.0% success rate. Since this data is slightly imblanaced, this would not be a good baseline for our model.\n",
    "\n",
    "One of the most important steps that we need to take is choosing a good evluation metric. The notebook that covers specific evaluation metrics can be located here: \n",
    "\n",
    "Accuracy does not make sense because of the imbalanced nature of the data. For this example we will use F1 score as our model evaluation metric.\n",
    "\n",
    "- F1 is calculated by 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "- Instead of a simple accuracy calculation which would give us a baseline of 96.1%, F1 score gives us an undefined number since both the precision and recall of a model that only predicted negatives would equal 0. \n",
    "\n",
    "- In this case, we want to use a simple basleline model like Naive Bayes to set our baseline based off of f1 score. You can use most models to create a baseline, but I like Naive bayes because it is quick and doesn't require much parameter tuning. (Full breakdown of Naieve Bayes in or Algorithms Course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ac71265-45a7-4fb1-b2c1-9d69176bc7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_accuracy: [0.74467535 0.74676413 0.7414113 ]\n",
      "nb F1_Macro Score: [0.40969243 0.41583439 0.41445341]\n",
      "nb_accuracy_avg: 0.7442835922580899  |  lr_f1_avg: 0.4133267459130525\n"
     ]
    }
   ],
   "source": [
    "# Import cross validation score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Import Naive Bayes Classifier \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create classifier object\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Run cv for NB classifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nb_accuracy = cross_val_score(nb,X_train,y_train.values.ravel(), cv=3, scoring ='accuracy')\n",
    "nb_f1 = cross_val_score(nb,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "\n",
    "print('nb_accuracy: ' +str(nb_accuracy))\n",
    "print('nb F1_Macro Score: '+str(nb_f1))\n",
    "print('nb_accuracy_avg: ' + str(nb_accuracy.mean()) +'  |  lr_f1_avg: '+str(nb_f1.mean()))\n",
    "\n",
    "\n",
    "# With these F1 scores, we can begin evaluating our model. While the accuracy is lower than if we only predicted 0 every time,\n",
    "# Our f1 score suggests we are doing a far better job of predicting stroke outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe470c-b81a-4738-9b40-cae7e22152ee",
   "metadata": {},
   "source": [
    "# Model Comparison & Selection \n",
    "After we have a baseline model to compare against, we want to evaluate how other models might perform on the same data. I like to experiment with other basic models with very little paramater tuning to see what performs well. This isn't an exact science and many people may do this step differently. After we set up the models, we can begin experimenting with parameter tuning. I find that model selection and parameter tuning is often an iterative process. For an analysis like this, trying different models, changing parameters, and experimenting with new engineered features is where I find myself spending most of my time working. \n",
    "\n",
    "In this section we will try:\n",
    "- Logistic regression\n",
    "- Decision Tree\n",
    "- K Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d425eb52-59dc-4c2d-a211-1310c3c8a8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt_accuracy: [0.82060365 0.82309812 0.82119764]\n",
      "dt F1_Macro Score: [0.27935131 0.27789764 0.27746978]\n",
      "dt_accuracy_avg: 0.8216331342755206  |  dt_f1_avg: 0.2782395764131652\n",
      "\n",
      "lr_accuracy: [0.63564087 0.68287883 0.4755693 ]\n",
      "lr F1_Macro Score: [0.39506357 0.41276915 0.24206472]\n",
      "lr_accuracy_avg: 0.5980296667946023  |  lr_f1_avg: 0.34996581243326824\n",
      "\n",
      "knn_accuracy: [0.84465736 0.84592634 0.84531909]\n",
      "knn F1_Macro Score: [0.2362048  0.24182392 0.2459295 ]\n",
      "knn_accuracy_avg: 0.8453009290324972  |  knn_f1_avg: 0.2413194051813984\n"
     ]
    }
   ],
   "source": [
    "#Let's now experiment with a few different basic models \n",
    "\n",
    "## Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state =32)\n",
    "\n",
    "dt_accuracy = cross_val_score(dt,X_train,y_train.values.ravel(), cv=3, scoring ='accuracy')\n",
    "dt_f1 = cross_val_score(dt,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "\n",
    "print('dt_accuracy: ' +str(dt_accuracy))\n",
    "print('dt F1_Macro Score: '+str(dt_f1))\n",
    "print('dt_accuracy_avg: ' + str(dt_accuracy.mean()) +'  |  dt_f1_avg: '+str(dt_f1.mean())+'\\n')\n",
    "\n",
    "\n",
    "## Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=32, max_iter = 2000, class_weight = 'balanced')\n",
    "\n",
    "lr_accuracy = cross_val_score(lr,X_train,y_train.values.ravel(), cv=3, scoring ='accuracy')\n",
    "lr_f1 = cross_val_score(lr,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "\n",
    "print('lr_accuracy: ' +str(lr_accuracy))\n",
    "print('lr F1_Macro Score: '+str(lr_f1))\n",
    "print('lr_accuracy_avg: ' + str(lr_accuracy.mean()) +'  |  lr_f1_avg: '+str(lr_f1.mean())+'\\n')\n",
    "\n",
    "\n",
    "## KNN \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "knn = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=3))\n",
    "knn_accuracy = cross_val_score(knn,X_train,y_train.values.ravel(), cv=3, scoring ='accuracy')\n",
    "knn_f1 = cross_val_score(knn,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "\n",
    "print('knn_accuracy: ' +str(knn_accuracy))\n",
    "print('knn F1_Macro Score: '+str(knn_f1))\n",
    "print('knn_accuracy_avg: ' + str(knn_accuracy.mean()) +'  |  knn_f1_avg: '+str(knn_f1.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd5e97-321c-40ba-a515-5cab302f2840",
   "metadata": {},
   "source": [
    "# Model Comparison \n",
    "It looks like we chose a pretty good baseline. While it slightly underperforms all of our new models in accuracy, it outperforms all of them in F1 score which is what we care about most for this analysis. Let's look at how everything stacks up. \n",
    "\n",
    "|Model          | F1 Score      |\n",
    "| :------------ | :-----------: |\n",
    "| **Baseline Naive Bayes**  | **41.3%**     |\n",
    "| Logistic Regression  | **35.0%**     |\n",
    "| Decision Tree  | **27.6%**     |\n",
    "| K Nearest Neighbors | **24.6%**     |\n",
    "\n",
    "\n",
    "\n",
    "While all of our models outperformed our basline, we still can do better. We can now parameter tune! That means that we make adjustments to the model parameter inputs to better compensate for our specific data. One of the drawbacks of Naive Bayes is that it has virtually no paramaters that we can tune, so our inital results are about the best we will get with it without making changes to our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f21094-dd66-4fa4-8a57-c3884dfa15fa",
   "metadata": {},
   "source": [
    "# Method 1: Manual Parameter Tuning\n",
    "Let's try to do some parameter tuning with a few of these models:\n",
    "\n",
    "Let's start with K Nearest Neighbors,which has a few parameters we can adjust, one of them being the number of K. K is how many other datapoints it uses to make its classification. If k= 3 it uses it sees what the samples 3 closest neighbors is and classifies it as the most common one. If k = 5, it uses its 5 closest datapoints. Let's change the number of k and see if that changes our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "229e4a29-dce4-4097-bb34-a468b9d67890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K =1: 0.27592578336190404\n",
      "K =2: 0.1292229598422949\n",
      "K =3: 0.2413194051813984\n",
      "K =4: 0.12708959225119731\n",
      "K =5: 0.20576233671467703\n",
      "K =6: 0.11913837004323076\n"
     ]
    }
   ],
   "source": [
    "#Knn Model Comparison \n",
    "\n",
    "#here we will loop through and see which value of k performs the best. \n",
    "\n",
    "for i in range(1,7):\n",
    "    knn = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=i))\n",
    "    knn_f1 = cross_val_score(knn,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "    print('K ='+(str(i)) + (': ') + str(knn_f1.mean()))\n",
    "\n",
    "#What we find is that k=1 is the best estimator for this specific model. We go from 24.6% to 27.6%, a decent improvement! \n",
    "#We also realize that KNN may not be the best approach here because of the imbalanced data. \n",
    "#The larger the K is, the more of the majority class will automatically be included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594fec2-43df-45cd-9b82-94d50bee892f",
   "metadata": {},
   "source": [
    "# Method 2: Randomized Parameter Tuning\n",
    "Since KNN may not be the best choice, let's explore the deicision tree. Decision trees have a lot more features we can tune. We can tweak the following:\n",
    "- criterion {gini, entropy, log loss}\n",
    "- splitter {best, random}\n",
    "- max depth {int, None}\n",
    "- min_samples_split {int, None}\n",
    "- min_samples_leave {int, None}\n",
    "- min_weight_fraction_leaf {float}\n",
    "- max_features {int, auto, sqrt, log2, None}\n",
    "- max_leaf_nodes {int, None}\n",
    "- min_impurity_decrease {float}\n",
    "- class_weight {dict, balanced, None}\n",
    "- ccp_alpha {float}\n",
    "\n",
    "There are a lot of parameters to tune! If there are just 2 options for each one that would be 2^11, which is 2048 total configurations. In theory, there are infinate numbers of paramater configurations. How do we even get close to finding the best one? \n",
    "\n",
    "The answer here is randomized search. We through in all the parameters that we are interested in searching, and the model will randomly select a subset and return the one that produces the best results. \n",
    "\n",
    "Still, let's manually select a few paramaters we want to evaluate on and then use randomized search:\n",
    "- criterion\n",
    "- split strategy\n",
    "- max depth\n",
    "- min_samples_split\n",
    "- max features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08b40592-ac8e-4252-aeb2-8a23dcb5b7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best stcore = 0.27862542627957537\n",
      "best params = {'splitter': 'random', 'min_samples_split': 2, 'max_features': None, 'max_depth': 40, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state = 42)\n",
    "\n",
    "features = {'criterion': ['gini','entropy'],\n",
    "            'splitter': ['best','random'],\n",
    "           'max_depth': [2,5,10,20,40,None],\n",
    "           'min_samples_split': [2,5,10,15],\n",
    "           'max_features': ['auto','sqrt','log2',None]}\n",
    "\n",
    "rs_dt = RandomizedSearchCV(estimator = dt, param_distributions =features, n_iter =100, cv = 3, random_state = 42, scoring ='f1')\n",
    "\n",
    "rs_dt.fit(X_train,y_train)\n",
    "\n",
    "print('best stcore = ' + str(rs_dt.best_score_))\n",
    "print('best params = ' + str(rs_dt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad4e647-1c56-401c-9348-0c4eebb414fd",
   "metadata": {},
   "source": [
    "# Method 3: GridsearchCV (Exhaustive Parameter Tuning)\n",
    "With this we have improved our model f1 score from **27.6% to 27.9%**. This is a decent increase! We also narrowed down some of the features that produced good results. We may want to try a more exhaustive search this time. Gridsearch goes through all of the possible combinations within an range and returns the best outcome. \n",
    "\n",
    "This time, let's do an exhaustive search of a smaller number of features and see if we can improve our results even more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a86427f0-c343-4c83-ae85-a776000b533c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best stcore = 0.2825002638608976\n",
      "best params = {'criterion': 'entropy', 'max_depth': 44, 'max_features': None, 'min_samples_split': 2, 'splitter': 'random'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "features_gs = {'criterion': ['entropy'],\n",
    "            'splitter': ['random'],\n",
    "           'max_depth': np.arange(30,50,1), #getting more precise within range\n",
    "           'min_samples_split': [2,3,4,5,6,7,8,9],\n",
    "           'max_features': [None]}\n",
    "\n",
    "gs_dt = GridSearchCV(estimator = dt, param_grid =features_gs, cv = 3, scoring ='f1') #we don't need random state because there isn't randomization like before\n",
    "\n",
    "gs_dt.fit(X_train,y_train)\n",
    "\n",
    "print('best stcore = ' + str(gs_dt.best_score_))\n",
    "print('best params = ' + str(gs_dt.best_params_))\n",
    "\n",
    "#looks like we can  do a little better with this gridsearch! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5049947-de2c-4b39-adef-287cac6b99f6",
   "metadata": {},
   "source": [
    "# Method 4: Bayesian Optimization\n",
    "I wonnder if we can do better than the funnel approach that we took with random search and gridsearch. What if we used a slightly smarter algorithm to help evaluate our features. Maybe we could explore all of the variables from the previous examples and see if our model missed something. This is where Bayesian Optimization comes in. This is an iterative process where our model improves its understandings of the feature inputs as it goes. (Full breakdown in the video portion of the course)\n",
    "\n",
    "Now let's try to use this with a larger feature set on the same classifier. This won't guarantee a better result as it still is not an exahustive search, but in theory it let's us cover ground in a more efficient way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e544990-a38c-438f-882e-98929e2ed60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/Users/emadsoheili/opt/anaconda3/envs/pythondata/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best stcore = 0.280078072106392\n",
      "best params = OrderedDict([('criterion', 'entropy'), ('max_depth', 50), ('max_features', None), ('min_samples_split', 2), ('splitter', 'random')])\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# Choose cross validation method \n",
    "cv = StratifiedKFold(n_splits = 3)\n",
    "\n",
    "\n",
    "bs_lr = BayesSearchCV(\n",
    "    dt,\n",
    "    {'criterion': Categorical(['gini','entropy']),\n",
    "            'splitter': Categorical(['best','random']),\n",
    "           'max_depth': Integer(10,50),\n",
    "           'min_samples_split': Integer(2,15),\n",
    "           'max_features': Categorical(['auto','sqrt','log2',None])},\n",
    "    random_state=42,\n",
    "    n_iter= 100,\n",
    "    cv= cv,\n",
    "    scoring ='f1')\n",
    " \n",
    "bs_lr.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "print('best stcore = ' + str(bs_lr.best_score_))\n",
    "print('best params = ' + str(bs_lr.best_params_))\n",
    "\n",
    "#while this didn't outperform our gridsearch, it is still a good approach to try when dealing with many different feature options. \n",
    "#it still did outperform our originial random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400ad743-419c-49b4-b37e-1f4392e0f11e",
   "metadata": {},
   "source": [
    "# Method 5: Selecting a Model\n",
    "We still haven't been able to do better than our baseline. In most cases, we to tune multiple different models until we reach one that performs the best based on our evaluation criteria. We also want to use other considerations like training time, prediction time, prediction time or interperetability to select selct the best model for our use case. \n",
    "\n",
    "Since we have one tuned model, lets see if we can improve it by combining it with a few of the other models we have used. This process is called ensembling. In the case of classification, we often use a popular vote metric to select the best model. \n",
    "\n",
    "Let's see if an ensemble model of these three classifiers outperforms our baseline model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "389eb114-8a42-4e88-be9b-c5f531d96964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "dt_voting = DecisionTreeClassifier(**{'criterion': 'entropy', 'max_depth': 44, 'max_features': None, 'min_samples_split': 2, 'splitter': 'random'}) # ** allows you to pass in parameters as dict\n",
    "knn_voting = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=1))\n",
    "lr_voting = LogisticRegression(random_state=32, max_iter = 2000, class_weight = 'balanced')\n",
    "\n",
    "ens = VotingClassifier(estimators = [('dt', dt_voting), ('knn', knn_voting), ('lr',lr_voting)], voting = 'hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eef3c062-3777-4dbc-a85a-58b818f1f116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voting_accuracy: [0.79715719 0.80551026 0.81436042]\n",
      "voting F1_Macro Score: [0.32528714 0.33004077 0.29857197]\n",
      "voting_accuracy_avg: 0.8056759548039101  |  voting_f1_avg: 0.31796662613905396\n"
     ]
    }
   ],
   "source": [
    "voting_accuracy = cross_val_score(ens,X_train,y_train.values.ravel(), cv=3, scoring ='accuracy')\n",
    "voting_f1 = cross_val_score(ens,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "\n",
    "print('voting_accuracy: ' +str(voting_accuracy))\n",
    "print('voting F1_Macro Score: '+str(voting_f1))\n",
    "print('voting_accuracy_avg: ' + str(voting_accuracy.mean()) +'  |  voting_f1_avg: '+str(voting_f1.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cfbd33d-4c52-4a04-ac97-09f495ed30c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voting_accuracy: [0.7980793  0.80619623 0.81187518]\n",
      "voting F1_Macro Score: [0.32543754 0.33317815 0.30153611]\n",
      "voting_accuracy_avg: 0.8053835701272538  |  voting_f1_avg: 0.3200505997609603\n"
     ]
    }
   ],
   "source": [
    "ens = VotingClassifier(estimators = [('dt', dt_voting), ('knn', knn_voting), ('lr',lr_voting)], voting = 'soft')\n",
    "voting_accuracy = cross_val_score(ens,X_train,y_train.values.ravel(), cv=3, scoring ='accuracy')\n",
    "voting_f1 = cross_val_score(ens,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "\n",
    "print('voting_accuracy: ' +str(voting_accuracy))\n",
    "print('voting F1_Macro Score: '+str(voting_f1))\n",
    "print('voting_accuracy_avg: ' + str(voting_accuracy.mean()) +'  |  voting_f1_avg: '+str(voting_f1.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ffb07-08c8-48e0-87b8-d9e36e5b1e27",
   "metadata": {},
   "source": [
    "# Stacked classifier \n",
    "In the case of the voting classifer, we didn't get better performance than our baseline model. Let's now try another type of ensembling called stacking. With stacking, we use the outputs of each of our individual models as features into a new model. In this case, where we have a decision tree, a naive baayes classifier, and a svc classifier, these will be the three features that a new model predicts on. \n",
    "\n",
    "Let's try running these three through a Naive Bayes Classifier and see what the results look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcc128e1-8efc-4293-8fe7-23100439e95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacking_accuracy: [0.73669118 0.7618555  0.81430419]\n",
      "stacking F1_Macro Score: [0.41848447 0.39380059 0.3119903 ]\n",
      "stacking_accuracy_avg: 0.770950288853223  |  stack_f1_avg: 0.37475845427999643\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "ens_stack = StackingClassifier(estimators = [('dt', dt_voting), ('lr',lr_voting), ('nb',GaussianNB())], final_estimator = GaussianNB())\n",
    "\n",
    "stack_accuracy = cross_val_score(ens_stack,X_train,y_train.values.ravel(), cv=3, scoring ='accuracy')\n",
    "stack_f1 = cross_val_score(ens_stack,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "\n",
    "print('stacking_accuracy: ' +str(stack_accuracy))\n",
    "print('stacking F1_Macro Score: '+str(stack_f1))\n",
    "print('stacking_accuracy_avg: ' + str(stack_accuracy.mean()) +'  |  stack_f1_avg: '+str(stack_f1.mean()))\n",
    "\n",
    "#in this case it didn't outperfrom, but it often does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d5881-34a5-4bb4-ac68-415c75dd4b07",
   "metadata": {},
   "source": [
    "# Ensemble Models\n",
    "The last main type of ensemble approach that we see is one that is designed that way algorithmically. Typically, random forest or gradient boosted models have ensembling built into their implementation. Let's explor random forest and see how this approach works for our data. (We have a breakdown of the main ensembling techniques in our full course on algorithms). These algorithms leverage multiple decision trees to either vote or give pass information on to subsequent models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6622226-b8db-43ee-8716-a38a790e0fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_accuracy: [0.85270899 0.85548496 0.85238122]\n",
      "rf F1_Macro Score: [0.19575095 0.20284101 0.20312026]\n",
      "rf_accuracy_avg: 0.8535250577865469  |  rf_f1_avg: 0.2005707413179846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#first let's try a non-tuned implementation \n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_accuracy = cross_val_score(rf,X_train,y_train.values.ravel(), cv=3, scoring ='accuracy')\n",
    "rf_f1 = cross_val_score(rf,X_train,y_train.values.ravel(), cv=3, scoring ='f1')\n",
    "\n",
    "print('rf_accuracy: ' +str(rf_accuracy))\n",
    "print('rf F1_Macro Score: '+str(rf_f1))\n",
    "print('rf_accuracy_avg: ' + str(rf_accuracy.mean()) +'  |  rf_f1_avg: '+str(rf_f1.mean()))\n",
    "\n",
    "#of course, you can tune this model like the others! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a2eb28a-0d1e-4abb-b2e4-28d19ff67216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None]\n"
     ]
    }
   ],
   "source": [
    "print([int(x) for x in np.linspace(10, 110, num = 11)]+[None])#.append(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5756ce-3787-42d1-91f9-e9fab732154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out because taking too long to run\n",
    "\n",
    "'''random_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)]+[None],\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'min_samples_leaf': [1, 2, 4],\n",
    "               'bootstrap': [True, False]}\n",
    "rs_rf = RandomizedSearchCV(estimator = rf, param_distributions =random_grid, n_iter =100, cv = 3, random_state = 42, scoring ='f1')\n",
    "\n",
    "rs_rf.fit(X_train,y_train.values.ravel())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb9e43c4-224c-480b-9144-9dc1e7a38ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline score 0.416924164394628\n",
      "dt score 0.2801016209253229\n",
      "voting score 0.3315657101299961\n",
      "Stacking score 0.4288173194614443\n",
      "rf score 0.20410096131079225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "nb.fit(X_train,y_train.values.ravel())\n",
    "ens.fit(X_train,y_train.values.ravel())\n",
    "dt_voting.fit(X_train,y_train.values.ravel())\n",
    "ens_stack.fit(X_train,y_train.values.ravel())\n",
    "rf_est = RandomForestClassifier()\n",
    "rf_est.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "nb_pred = nb.predict(X_test)\n",
    "ens_pred = ens.predict(X_test)\n",
    "dt_pred = dt_voting.predict(X_test)\n",
    "ens_stack_pred = ens_stack.predict(X_test)\n",
    "rf_pred = rf_est.predict(X_test)\n",
    "\n",
    "print('baseline score ' + str(f1_score(y_test,nb_pred)))\n",
    "print('dt score ' + str(f1_score(y_test,dt_pred)))\n",
    "print('voting score ' + str(f1_score(y_test,ens_pred)))\n",
    "print('Stacking score ' + str(f1_score(y_test,ens_stack_pred)))\n",
    "print('rf score ' + str(f1_score(y_test,rf_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
